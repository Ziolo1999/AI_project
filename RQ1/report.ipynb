{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/karol/Desktop/Antwerp/ai_project\")\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import MLP1, TwoTower, TwoTowerBasic\n",
    "from data_reader import load_data, data_preprocessing, load_data_mf, load_customers_articles, customer_buckets, matrix_representation, create_random_candidates\n",
    "from helper import validate_softmax,  train_softmax, train_two_tower\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from recommenders import recommender_softmax, recommender_two_towers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "- Most important outcomes from feature engineering part were applied in data preprocessing function.\n",
    "- Function split_transaction generates targets (last purchases are considered as targets).\n",
    "- Function atrix factorization transform transaction to pivot matrix.\n",
    "- Create specific Dataset classes for specific task.\n",
    "- Use sparse matrices to handle sparse data (create collate_fn for DataLoaders)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions, articles, customers, article_encodings, customer_encodings, article_decodings, customer_decodings = data_preprocessing(feature_generation=False, return_encodings=True, save=True)\n",
    "# transactions_candidates = create_random_candidates(transactions, save_dir=None, num_sample=30_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data and apply one hot encoding for articles and customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "transactions = pd.read_csv(\"data/preprocessed/transactions.csv\") \n",
    "articles = pd.read_csv(\"data/preprocessed/articles.csv\") \n",
    "customers = pd.read_csv(\"data/preprocessed/customers.csv\") \n",
    "\n",
    "# one hot encoding \n",
    "articles = articles.set_index(\"article_id\")\n",
    "customers = customers.set_index(\"customer_id\")\n",
    "\n",
    "article_enc = OneHotEncoder(sparse_output=True)\n",
    "articles = article_enc.fit_transform(articles)\n",
    "\n",
    "customers_categorical = [\"FN\",'Active',\"club_member_status\", \"fashion_news_frequency\"]\n",
    "customers_cont = [\"age\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(sparse_output=True), customers_categorical),\n",
    "        ('cont', 'passthrough', customers_cont)  # 'passthrough' means no transformation for continuous variables\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns not explicitly transformed\n",
    ")\n",
    "customers = csr_matrix(preprocessor.fit_transform(customers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "- The input is the purchase history without last purchase,\n",
    "- The target is the basket of the last purchase,\n",
    "- We are interested in whether article was bought not its amount. Therefore, the binary values are allowed.\n",
    "- In the target basket multiple articles could be bought therefore the sigmoid activation function was used at the last layer. It assumes that decision to buy specific article is independent from other products that customer is buying.\n",
    "- As I have multi-classification problem and I use sigmoid asa final activation layer I decided to use BCEWithLogitsLoss which supposed to deal with this configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = load_data(transactions, train_test=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & Train basic Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [17:28<2:37:15, 1048.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [34:39<2:18:26, 1038.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [51:37<2:00:02, 1028.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [1:08:35<1:42:28, 1024.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [1:25:33<1:25:10, 1022.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [1:42:28<1:07:59, 1019.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [1:59:24<50:56, 1018.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [2:16:20<33:55, 1017.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [2:33:17<16:57, 1017.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [2:50:11<00:00, 1021.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] - Train Loss: 0.6931, Validation Loss: 0.6931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = MLP1(input_dim=articles.shape[0], output_dim=articles.shape[0])\n",
    "# get params\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "save_dir = \"AI_project/RQ1/models/MLP1.pt\"\n",
    "# train\n",
    "val_loss_MLP = train_softmax(model, train_dataloader, val_dataloader, criterion, optimizer, save_dir, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP1 = torch.load(\"AI_project/RQ1/models/MLP1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/data_reader.py:215: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  i = torch.LongTensor(indices)\n",
      "/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/data_reader.py:219: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:607.)\n",
      "  return torch.sparse.FloatTensor(i, v, s)\n",
      "/Users/karol/.pyenv/versions/3.9.17/envs/ai_project/lib/python3.9/site-packages/torch/_tensor_str.py:137: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0028, device='mps:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations, accuracy = recommender_softmax(MLP1, val_dataloader, evaluate=True)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for improvements:\n",
    "- No warm start\n",
    "- Model should be trained on customers who had at least two purchases\n",
    "- We take into account baskets which are based on the past 2 years. Maybe we should train model based on customers who bought articles in last month.\n",
    "- Different way to measure accuracy.\n",
    "- Customers whou didn't developed any patterns (bought low amount of clothes) should be recommended with current top selling articles.\n",
    "- Develop more complex (deeper) model.\n",
    "- More training.\n",
    "- Distinguish customers who buys specific articles multiple times.\n",
    "### Potential Issues:\n",
    "- We need to predict articles for customers who were used to train the model.\n",
    "\n",
    "**Idea**: recommend things that haven't been bought.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization with DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "- Firstly, it was decided to use one hot encoding for all categorical features.\n",
    "- Two tower architecture was used which is compsed from two different classes, which are responsible for encoding customer and article features. \n",
    "- These models are used for estimating embeddings for recommendations.\n",
    "- To estimate the probability of buying article x by customer y the product between corresponding embbedings is calculated and then the sigmoid function is applied. \n",
    "- For training purposes the random negative candidates have been generated.\n",
    "- Weights for recent articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_candidates = pd.read_csv(\"data/preprocessed/transactions_candidates.csv\")\n",
    "train_dataloader, val_dataloader, test_customers = load_data_mf(transactions_candidates, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:57<00:00, 71.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.1969, Validation Loss: 0.1761%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:57<00:00, 71.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] - Train Loss: 0.1975, Validation Loss: 0.1760%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:58<00:00, 71.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] - Train Loss: 0.1971, Validation Loss: 0.1760%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:57<00:00, 71.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] - Train Loss: 0.1970, Validation Loss: 0.1760%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:58<00:00, 71.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] - Train Loss: 0.1969, Validation Loss: 0.1760%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:58<00:00, 71.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] - Train Loss: 0.1971, Validation Loss: 0.1759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:58<00:00, 71.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] - Train Loss: 0.1975, Validation Loss: 0.1759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:57<00:00, 71.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] - Train Loss: 0.1976, Validation Loss: 0.1759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:58<00:00, 71.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] - Train Loss: 0.1977, Validation Loss: 0.1759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55595/55595 [12:58<00:00, 71.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] - Train Loss: 0.1978, Validation Loss: 0.1759%\n"
     ]
    }
   ],
   "source": [
    "input_article_dim = articles.shape[1]\n",
    "input_customer_dim = customers.shape[1]\n",
    "model = TwoTower(input_article_dim, input_customer_dim, output_dim=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "save_dir = \"AI_project/RQ1/models/TwoTower1.pt\"\n",
    "val_loss_tower = train_two_tower(model, customers, articles, train_dataloader, val_dataloader, criterion, optimizer, save_dir, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoTower1 = torch.load(\"AI_project/RQ1/models/TwoTower1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data required for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_full = matrix_representation(transactions_candidates, train_test=False)\n",
    "targets = matrix_full[test_customers]\n",
    "dataloader_cust, dataloader_art = load_customers_articles(customers, articles, test_customers=test_customers, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Customer Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1363/1363 [00:06<00:00, 222.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Articles Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1056/1056 [00:04<00:00, 249.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get recommendations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:27<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "recommendations, accuracy = recommender_two_towers(TwoTower1, dataloader_cust, dataloader_art, targets, evaluate=True, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for improvements:\n",
    "- Use embeddings for warm start\n",
    "- Develop ebedding layers.\n",
    "- Generate more features.\n",
    "- More training.\n",
    "- Distinguish customers who buys specific articles multiple times.\n",
    "- Use article embeddings for recommendations.\n",
    "- Mix ways of recommending things\n",
    "### Potential Issues:\n",
    "- We need to predict articles for customers who were used to train the model.\n",
    "\n",
    "**Idea**: recommend things that haven't been bought.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Final Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv(\"data/preprocessed/transactions.csv\") \n",
    "dataloader = load_data(transactions, train_test=False, batch_size=1000)\n",
    "MLP1 = torch.load(\"AI_project/RQ1/models/MLP1.pt\")\n",
    "recommendations = recommender_softmax(MLP1, dataloader, evaluate=False, top_k=10).to(torch.int64).to(\"cpu\").numpy()\n",
    "output = pd.DataFrame(recommendations).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed/customers_decoding.pickle', 'rb') as file:\n",
    "    customer_dict = pickle.load(file)\n",
    "\n",
    "with open('data/preprocessed/articles_decoding.pickle', 'rb') as file:\n",
    "    article_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1,10,1):\n",
    "    output[i] = output[i].apply(lambda x: article_dict[\"article_id\"][x])\n",
    "preds = [' '.join(['0' + str(article_dict[\"article_id\"][p]) for p in ps]) for ps in recommendations]\n",
    "submission = pd.DataFrame(zip(np.arange(len(preds)), preds), columns=[\"customer_id\",\"prediction\"])\n",
    "submission[\"customer_id\"] = submission[\"customer_id\"].apply(lambda x: customer_dict[\"customer_id\"][x])\n",
    "submission.to_csv(\"submission/MLP/MLP1_submission.csv.gz\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Customer Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1372/1372 [01:04<00:00, 21.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Articles Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:03<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get recommendations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1372/1372 [04:49<00:00,  4.74it/s]\n"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_csv(\"data/preprocessed/transactions.csv\") \n",
    "dataloader_cust, dataloader_art = load_customers_articles(customers, articles, batch_size=1000)\n",
    "TwoTower = torch.load(\"AI_project/RQ1/models/TwoTower1.pt\")\n",
    "recommendations = recommender_two_towers(TwoTower, dataloader_cust, dataloader_art, targets=None, top_k=10).to(torch.int64).to(\"cpu\").numpy()\n",
    "output = pd.DataFrame(recommendations).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1,10,1):\n",
    "    output[i] = output[i].apply(lambda x: article_dict[\"article_id\"][x])\n",
    "    \n",
    "preds = [' '.join(['0' + str(article_dict[\"article_id\"][p]) for p in ps]) for ps in recommendations]\n",
    "submission = pd.DataFrame(zip(np.arange(len(preds)), preds), columns=[\"customer_id\",\"prediction\"])\n",
    "submission[\"customer_id\"] = submission[\"customer_id\"].apply(lambda x: customer_dict[\"customer_id\"][x])\n",
    "submission.to_csv(\"submission/TwoTower/TwoTower2_submission.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
