{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/karol/Desktop/Antwerp/ai_project\")\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import MLP1, TwoTower, TwoTowerBasic\n",
    "from data_reader import load_data, data_preprocessing, load_data_mf, load_customers_articles, customer_buckets\n",
    "from helper import validate_softmax,  train_softmax, train_two_tower\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from recommenders import recommender_softmax, recommender_two_towers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions, articles, customers, article_encodings, customer_encodings = data_preprocessing(feature_generation=False, return_encodings=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data and apply one hot encoding for articles and customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "transactions = pd.read_csv(\"data/preprocessed/transactions.csv\") \n",
    "articles = pd.read_csv(\"data/preprocessed/articles.csv\") \n",
    "customers = pd.read_csv(\"data/preprocessed/customers.csv\") \n",
    "\n",
    "# one hot encoding \n",
    "articles = articles.set_index(\"article_id\")\n",
    "customers = customers.set_index(\"customer_id\")\n",
    "\n",
    "article_enc = OneHotEncoder(sparse_output=True)\n",
    "articles = article_enc.fit_transform(articles)\n",
    "\n",
    "customers_categorical = [\"FN\",'Active',\"club_member_status\", \"fashion_news_frequency\"]\n",
    "customers_cont = [\"age\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(sparse_output=True), customers_categorical),\n",
    "        ('cont', 'passthrough', customers_cont)  # 'passthrough' means no transformation for continuous variables\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns not explicitly transformed\n",
    ")\n",
    "customers = csr_matrix(preprocessor.fit_transform(customers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "- The input is the purchase history without last purchase,\n",
    "- The target is the basket of the last purchase,\n",
    "- We are interested in whether article was bought not its amount. Therefore, the binary values are allowed.\n",
    "- In the target basket multiple articles could be bought therefore the sigmoid activation function was used at the last layer. It assumes that decision to buy specific article is independent from other products that customer is buying.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = load_data(transactions, train_test=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & Train basic Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [11:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/report.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/report.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m save_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAI_project/RQ1/models/MLP1.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/report.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/report.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m val_loss_list \u001b[39m=\u001b[39m train_softmax(model, train_dataloader, val_dataloader, criterion, optimizer, save_dir, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/Antwerp/ai_project/AI_project/RQ1/helper.py:31\u001b[0m, in \u001b[0;36mtrain_softmax\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, save_dir, num_epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         min_loss \u001b[39m=\u001b[39m val_loss\n\u001b[1;32m     30\u001b[0m         torch\u001b[39m.\u001b[39msave(model, save_dir)\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m] - Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Validation Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mval_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m val_loss_list\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# # create model\n",
    "# model = MLP1(input_dim=articles.shape[0], output_dim=articles.shape[0])\n",
    "# # get params\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# save_dir = \"AI_project/RQ1/models/MLP1.pt\"\n",
    "# # train\n",
    "# val_loss_list = train_softmax(model, train_dataloader, val_dataloader, criterion, optimizer, save_dir, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP1 = torch.load(\"AI_project/RQ1/models/MLP1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/data_reader.py:215: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  i = torch.LongTensor(indices)\n",
      "/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/data_reader.py:219: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:607.)\n",
      "  return torch.sparse.FloatTensor(i, v, s)\n",
      "/Users/karol/.pyenv/versions/3.9.17/envs/ai_project/lib/python3.9/site-packages/torch/_tensor_str.py:137: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0028, device='mps:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations, accuracy = recommender_softmax(MLP1, val_dataloader, evaluate=True)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for improvements:\n",
    "- No warm start\n",
    "- Model should be trained on customers who had at least two purchases\n",
    "- We take into account baskets which are based on the past 2 years. Maybe we should train model based on customers who bought articles in last month.\n",
    "- Different way to measure accuracy.\n",
    "- Customers whou didn't developed any patterns (bought low amount of clothes) should be recommended with current top selling articles.\n",
    "- Develop more complex (deeper) model.\n",
    "- More training.\n",
    "- Distinguish customers who buys specific articles multiple times.\n",
    "### Potential Issues:\n",
    "- We need to predict articles for customers who were used to train the model.\n",
    "\n",
    "**Idea**: recommend things that haven't been bought.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization with DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = customer_buckets(transactions, train_test=False)\n",
    "train_dataloader, val_dataloader, test_customers = load_data_mf(transactions, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 1369/28589 [00:16<05:26, 83.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/report.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/report.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/report.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m save_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAI_project/RQ1/models/TwoTower1.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/karol/Desktop/Antwerp/ai_project/AI_project/RQ1/report.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_two_tower(model, customers, articles, buckets, train_dataloader, val_dataloader, criterion, optimizer, save_dir, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/Antwerp/ai_project/AI_project/RQ1/helper.py:57\u001b[0m, in \u001b[0;36mtrain_two_tower\u001b[0;34m(model, customers, articles, buckets, train_dataloader, val_dataloader, criterion, optimizer, save_dir, num_epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     56\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mfor\u001b[39;00m articles_id, customers_id \u001b[39min\u001b[39;00m tqdm(train_dataloader): \n\u001b[1;32m     58\u001b[0m         \u001b[39m# Positive sample\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         articles_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(articles[articles_id]\u001b[39m.\u001b[39mtodense(), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     60\u001b[0m         customer_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(customers[customers_id]\u001b[39m.\u001b[39mtodense(), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/ai_project/lib/python3.9/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/ai_project/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/ai_project/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/ai_project/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/ai_project/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/Antwerp/ai_project/AI_project/RQ1/data_reader.py:245\u001b[0m, in \u001b[0;36mDatasetMF.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index:\u001b[39mint\u001b[39m):\n\u001b[1;32m    244\u001b[0m     article_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransactions[\u001b[39m\"\u001b[39m\u001b[39marticle_id\u001b[39m\u001b[39m\"\u001b[39m][index]\n\u001b[0;32m--> 245\u001b[0m     customer_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransactions[\u001b[39m\"\u001b[39;49m\u001b[39mcustomer_id\u001b[39;49m\u001b[39m\"\u001b[39;49m][index]\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m article_id, customer_id\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/ai_project/lib/python3.9/site-packages/pandas/core/series.py:1023\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mEllipsis\u001b[39m:\n\u001b[1;32m   1021\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m-> 1023\u001b[0m key_is_scalar \u001b[39m=\u001b[39m is_scalar(key)\n\u001b[1;32m   1024\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1025\u001b[0m     key \u001b[39m=\u001b[39m unpack_1tuple(key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_article_dim = articles.shape[1]\n",
    "input_customer_dim = customers.shape[1]\n",
    "model = TwoTowerBasic(input_article_dim, input_customer_dim, output_dim=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "save_dir = \"AI_project/RQ1/models/TwoTower1.pt\"\n",
    "train_two_tower(model, customers, articles, buckets, train_dataloader, val_dataloader, criterion, optimizer, save_dir, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoTower1 = torch.load(\"AI_project/RQ1/models/TwoTower1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_full = matrix_representation(transactions, train_test=False)\n",
    "targets = matrix_full[test_customers]\n",
    "dataloader_cust, dataloader_art = load_customers_articles(customers, articles, test_customers=test_customers, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations, accuracy = recommender_two_towers(TwoTower1, dataloader_cust, dataloader_art, targets, evaluate=True, top_k=5)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
